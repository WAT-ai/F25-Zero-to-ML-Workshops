{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Workshop 2: Data Loaders and Output Parsers\n",
    "\n",
    "In this notebook, we'll explore:\n",
    "1. Loading data from interesting sources (YouTube, news, web pages)\n",
    "2. Using output parsers to get structured responses\n",
    "3. Building practical applications that combine both\n",
    "\n",
    "## Setup: Loading Our API Keys Securely üîê\n",
    "\n",
    "First, let's load our environment variables. \n",
    "\n",
    "In real life, **NEVER** put API keys directly in your code! For this Kaggle workshop only, you can set your API key in the `KAGGLE_BACKUP` variable below. Though locally, you should use a `.env` file with the following content: \n",
    "```.env\n",
    "OPENAI_API_KEY=\"your-key\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only if running this on Kaggle\n",
    "KAGGLE_BACKUP = \"sk-...\"  # Replace with your OpenAI key for Kaggle only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import requests\n",
    "    import langchain\n",
    "    import langchain_openai\n",
    "    import langchain_community\n",
    "    from dotenv import load_dotenv\n",
    "    import bs4\n",
    "    import langchain_yt_dlp\n",
    "    import feedparser\n",
    "    import tqdm\n",
    "    from newspaper import Article\n",
    "except ImportError as e:\n",
    "    !pip install requests python-dotenv langchain-openai langchain langchain-community beautifulsoup4 feedparser langchain-yt-dlp newspaper3k listparser lxml-html-clean tqdm\n",
    "    import requests\n",
    "    import langchain\n",
    "    import langchain_openai\n",
    "    import langchain_community\n",
    "    from dotenv import load_dotenv\n",
    "    import bs4\n",
    "    import langchain_yt_dlp\n",
    "    import feedparser\n",
    "    from newspaper import Article\n",
    "    import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key loaded successfully: sk-proj-xTrb...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load API key with Kaggle backup\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", KAGGLE_BACKUP)\n",
    "if api_key:\n",
    "    print(f\"‚úÖ API key loaded successfully: {api_key[:12]}...\")\n",
    "else:\n",
    "    print(\"‚ùå No API key found. Make sure you have a .env file with OPENAI_API_KEY=\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano\", temperature=0.3, api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Loaders - Getting Data from the Wild\n",
    "\n",
    "Loading data from websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents\n",
      "Content length: 6828 characters\n",
      "\n",
      "Start of article:\n",
      "able of contents Frystown, Pennsylvania 13 languages ÿ™€Üÿ±⁄©ÿ¨ŸáCebuanoEspa√±olŸÅÿßÿ±ÿ≥€åFran√ßais⁄Ø€åŸÑ⁄©€åItalianoLadin–ù–æ—Ö—á–∏–π–Ω–°—Ä–ø—Å–∫–∏ / srpskiSrpskohrvatski / —Å—Ä–ø—Å–∫–æ—Ö—Ä–≤–∞—Ç—Å–∫–∏–¢–∞—Ç–∞—Ä—á–∞ / tatar√ßa–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ Edit links ArticleTalk English ReadEditView history Tools Tools move to sidebar hide Actions ReadEditView history General What links hereRelated changesUpload filePermanent linkPage informationCite this pageGet shortened URLDownload QR code Print/export Download as PDFPrintable version In other projects Wikimedia CommonsWikidata item Appearance move to sidebar hide Coordinates: 40¬∞26‚Ä≤59‚Ä≥N 76¬∞20‚Ä≤03‚Ä≥WÔªø / Ôªø40.44972¬∞N 76.33417¬∞WÔªø / 40.44972; -76.33417 From Wikipedia, the free encyclopedia Unincorporated community in Pennsylvania, US Census-designated place in Pennsylvania, United StatesFrystown, PennsylvaniaCensus-designated placeFrystownShow map of PennsylvaniaFrystownShow map of the United StatesCoordinates: 40¬∞26‚Ä≤59‚Ä≥N 76¬∞20‚Ä≤03‚Ä≥WÔªø / Ôªø40.44972¬∞N 76.33417¬∞WÔªø / 40.44972; -76.33417CountryUnited StatesStatePennsylvaniaCountyBerksTownshipBethelArea[1] ‚Ä¢ Total1.22 sq mi (3.17 km2) ‚Ä¢ Land1.21 sq mi (3.14 km2) ‚Ä¢ Water0.012 sq mi (0.03 km2)Elevation466 ft (142 m)Population (2020)[2] ‚Ä¢ Total355 ‚Ä¢ Density292.8/sq mi (113.04/km2)Time zoneUTC-5 (Eastern (EST)) ‚Ä¢ Summer (DST)UTC-4 (EDT)ZIP codes17067 & 19507Area code717FIPS code42-28128GNIS feature ID1175322[3] Frystown is a census-designated place[4] in Bethel Township, in far western Berks County, Pennsylvania, United States. It is located near the township line with Tulpehocken Township. The community is served by the Tulpehocken Area School District. As of the 2010 census, the population was 380 residents. The Little Swatara Creek forms the natural southern boundary of Frystown and flows westward into the Swatara Creek, a tributary of the Susquehanna River. Interstate 78 has an interchange with Route 645 in Frystown. The CDP is split between the Myerstown and Bethel post offices, which use the ZIP codes of 17067 and 19507, respectively.[5] Demographics[edit] Historical population CensusPop.Note%¬± 2020355‚ÄîU.S. Decennial Census[6] References[edit] ^ \"ArcGIS REST Services Directory\". United States Census Bureau. Retrieved October 12, 2022. ^ \"Census Population API\". United States Census Bureau. Retrieved Oct 12, 2022. ^ \"Frystown\". Geographic Names Information System. United States Geological Survey, United States Department of the Interior. ^ \"2010 Census\". Archived from the original on 2013-12-23. Retrieved 2013-02-10. ^ \"19507 ZIP Code P...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Load a Wikipedia page about something random \n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Special:Random\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Clean up page content to remove extra whitespace\n",
    "for doc in docs:\n",
    "    doc.page_content = \" \".join(doc.page_content.split())\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "print(f\"Content length: {len(docs[0].page_content)} characters\")\n",
    "print(\"\\nStart of article:\")\n",
    "print(docs[0].page_content[500:3000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is this article about?\n",
      "A: This article is about Frystown, Pennsylvania, an unincorporated community and census-designated place in Bethel Township, Berks County. It provides geographic and demographic details (location coordinates, area of about 1.22 sq mi, population 355 in 2020, ZIP codes 17067 and 19507) and information on local administration (Tulpehocken Area School District) and nearby features like Little Swatara Creek and Interstate 78.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question about the loaded content\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Use only start of article to keep token limits low. \n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Based on this content, answer in 1-4 sentences: {question}\\n\\nContent: {content}\"\n",
    ")\n",
    "\n",
    "question = \"What is this article about?\"\n",
    "response = llm.invoke(summary_prompt.format(\n",
    "    question=question,\n",
    "    content=docs[0].page_content[500:3000]\n",
    "))\n",
    "\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from Youtube videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YouTube description loaded: 2349 characters\n",
      "\n",
      "First 300 characters:\n",
      "The official video for ‚ÄúNever Gonna Give You Up‚Äù by Rick Astley. Never: The Autobiography üìö OUT NOW! Follow this link to get your copy and listen to Rick‚Äôs ‚ÄòNever‚Äô playlist ‚ù§Ô∏è #RickAstleyNever https://linktr.ee/rickastleynever ‚ÄúNever Gonna Give You Up‚Äù was a global smash on its release in July 1987,...\n",
      "{'source': 'dQw4w9WgXcQ', 'title': 'Rick Astley - Never Gonna Give You Up (Official Video) (4K Remaster)', 'description': 'The official video for ‚ÄúNever Gonna Give You Up‚Äù by Rick Astley. Never: The Autobiography üìö OUT NOW! Follow this link to get your copy and listen to Rick‚Äôs ‚ÄòNever‚Äô playlist ‚ù§Ô∏è #RickAstleyNever https://linktr.ee/rickastleynever ‚ÄúNever Gonna Give You Up‚Äù was a global smash on its release in July 1987, topping the charts in 25 countries including Rick‚Äôs native UK and the US Billboard Hot 100. It also won the Brit Award for Best single in 1988. Stock Aitken and Waterman wrote and produced the track which was the lead-off single and lead track from Rick‚Äôs debut LP ‚ÄúWhenever You Need Somebody‚Äù. The album was itself a UK number one and would go on to sell over 15 million copies worldwide. The legendary video was directed by Simon West ‚Äì who later went on to make Hollywood blockbusters such as Con Air, Lara Croft ‚Äì Tomb Raider and The Expendables 2. The video passed the 1bn YouTube views milestone on 28 July 2021. Subscribe to the official Rick Astley YouTube channel: https://RickAstley.lnk.to/YTSubID Follow Rick Astley: Facebook: https://RickAstley.lnk.to/FBFollowID Twitter: https://RickAstley.lnk.to/TwitterID Instagram: https://RickAstley.lnk.to/InstagramID Website: https://RickAstley.lnk.to/storeID TikTok: https://RickAstley.lnk.to/TikTokID Listen to Rick Astley: Spotify: https://RickAstley.lnk.to/SpotifyID Apple Music: https://RickAstley.lnk.to/AppleMusicID Amazon Music: https://RickAstley.lnk.to/AmazonMusicID Deezer: https://RickAstley.lnk.to/DeezerID Lyrics: We‚Äôre no strangers to love You know the rules and so do I A full commitment‚Äôs what I‚Äôm thinking of You wouldn‚Äôt get this from any other guy I just wanna tell you how I‚Äôm feeling Gotta make you understand Never gonna give you up Never gonna let you down Never gonna run around and desert you Never gonna make you cry Never gonna say goodbye Never gonna tell a lie and hurt you We‚Äôve known each other for so long Your heart‚Äôs been aching but you‚Äôre too shy to say it Inside we both know what‚Äôs been going on We know the game and we‚Äôre gonna play it And if you ask me how I‚Äôm feeling Don‚Äôt tell me you‚Äôre too blind to see Never gonna give you up Never gonna let you down Never gonna run around and desert you Never gonna make you cry Never gonna say goodbye Never gonna tell a lie and hurt you #RickAstley #NeverGonnaGiveYouUp #WheneverYouNeedSomebody #OfficialMusicVideo', 'view_count': 1701365887, 'publish_date': '2009-10-25', 'length': 213, 'author': 'Rick Astley', 'channel_id': 'UCuAXFkgsw1L7xaCfnd5JJOw', 'webpage_url': 'https://www.youtube.com/watch?v=dQw4w9WgXcQ'}\n"
     ]
    }
   ],
   "source": [
    "# YouTube transcript loader (requires youtube-transcript-api)\n",
    "from langchain_yt_dlp.youtube_loader import YoutubeLoaderDL\n",
    "\n",
    "# Load any Youtube video\n",
    "youtube_loader = YoutubeLoaderDL.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",  # Replace with actual video\n",
    "    add_video_info=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    youtube_docs = youtube_loader.load()\n",
    "\n",
    "    for doc in youtube_docs:\n",
    "        doc.metadata['description'] = \" \".join(doc.metadata['description'].split())\n",
    "\n",
    "    print(f\"YouTube description loaded: {len(youtube_docs[0].metadata['description'])} characters\")\n",
    "    print(\"\\nFirst 300 characters:\")\n",
    "    print(youtube_docs[0].metadata['description'][:300] + \"...\")\n",
    "    print(youtube_docs[0].metadata)\n",
    "except Exception as e:\n",
    "    print(f\"YouTube loading failed (this is common): {e}\")\n",
    "    print(\"We'll use web content instead for the exercises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Summarize the main topic of this video in one sentence.\n",
      "A: This is the official music video for Rick Astley's 1987 hit \"Never Gonna Give You Up,\" highlighting its release, chart-topping success, production by Stock Aitken and Waterman, and its enduring popularity.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question if YouTube loading succeeded\n",
    "if 'youtube_docs' in locals() and youtube_docs:\n",
    "    question = \"Summarize the main topic of this video in one sentence.\"\n",
    "    response = llm.invoke(summary_prompt.format(\n",
    "        question=question,\n",
    "        content=youtube_docs[0].metadata['description']\n",
    "    ))\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {response.content}\")\n",
    "else:\n",
    "    print(\"Skipping YouTube Q&A since loading failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:16,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 news articles\n",
      "\n",
      "First article title: Don't force drivers to use parking apps, RAC says\n",
      "Summary: Don't force drivers to use parking apps, RAC says The RAC said paying for parking with an app should not be the only option for drivers The RAC welcomed the NPP but said more local authorities and par...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# RSS/News loader\n",
    "from langchain_community.document_loaders import RSSFeedLoader\n",
    "\n",
    "# Load recent news\n",
    "rss_loader = RSSFeedLoader(\n",
    "    urls=[\"https://feeds.bbci.co.uk/news/business/rss.xml?edition=int\"],\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    news_docs = rss_loader.load()\n",
    "    for doc in news_docs:\n",
    "        doc.page_content = \" \".join(doc.page_content.split())\n",
    "\n",
    "    print(f\"Loaded {len(news_docs)} news articles\")\n",
    "    print(\"\\nFirst article title:\", news_docs[0].metadata.get('title', 'No title'))\n",
    "    print(\"Summary:\", news_docs[0].page_content[:200] + \"...\")\n",
    "except Exception as e:\n",
    "    print(f\"RSS loading failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What are the main themes in these articles? Use examples from the articles to justify your response.\n",
      "A: The main themes are policy decisions and their real-world effects across technology, taxation, and international finance. In parking, the RAC warns against forcing drivers to use parking apps and notes issues like poor phone signal and the app not recognizing car parks, even as it welcomes the National Parking Platform. In housing, the debate over abolishing stamp duty shows how tax policy could reshape the housing market and political calculations around it. Internationally, the US move to implement a controversial rescue plan for Argentina illustrates how policy interventions are used to address currency crises.\n"
     ]
    }
   ],
   "source": [
    "# Ask about the news articles if loading succeeded\n",
    "if 'news_docs' in locals() and news_docs:\n",
    "    # Combine first 3 article titles and summaries, truncated\n",
    "    news_sample = \"\\n\".join([\n",
    "        f\"{doc.metadata.get('title', 'No title')}: {doc.page_content[:500]}\" \n",
    "        for doc in news_docs[:3]\n",
    "    ])\n",
    "    \n",
    "    question = \"What are the main themes in these articles? Use examples from the articles to justify your response.\"\n",
    "    response = llm.invoke(summary_prompt.format(\n",
    "        question=question,\n",
    "        content=news_sample\n",
    "    ))\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {response.content}\")\n",
    "else:\n",
    "    print(\"Skipping RSS Q&A since loading failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Output Parsers - Getting Structured Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What the LLM received Write a review for the movie 'The Matrix but everyone is a rubber duck'.\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"title\": {\"description\": \"Movie title\", \"title\": \"Title\", \"type\": \"string\"}, \"rating\": {\"description\": \"Rating from 1-10\", \"title\": \"Rating\", \"type\": \"integer\"}, \"pros\": {\"description\": \"List of positive aspects\", \"items\": {\"type\": \"string\"}, \"title\": \"Pros\", \"type\": \"array\"}, \"cons\": {\"description\": \"List of negative aspects\", \"items\": {\"type\": \"string\"}, \"title\": \"Cons\", \"type\": \"array\"}, \"recommended\": {\"description\": \"Whether you'd recommend it\", \"title\": \"Recommended\", \"type\": \"boolean\"}}, \"required\": [\"title\", \"rating\", \"pros\", \"cons\", \"recommended\"]}\n",
      "```\n",
      "Structured review:\n",
      "Title: The Matrix but everyone is a rubber duck\n",
      "Rating: 7/10\n",
      "Pros: ['Inventive premise that turns every scene into a squeaky showdown', 'Delightful rubber duck designs, squeaks, and bath-time aesthetics', 'Parody that respects The Matrix while leaning into whimsy', 'Witty, punny humor that lands more often than it misses', 'Thoughtful moments about identity and control delivered with rubber charm']\n",
      "Cons: ['Some action sequences slow down as jokes take precedence over momentum', 'The constant quacking can wear thin for viewers not fond of puns', 'A few CGI rubber models look budgetier in busy set pieces']\n",
      "Recommended: True\n"
     ]
    }
   ],
   "source": [
    "# Structured data with Pydantic\n",
    "class MovieReview(BaseModel):\n",
    "    title: str = Field(description=\"Movie title\")\n",
    "    rating: int = Field(description=\"Rating from 1-10\")\n",
    "    pros: List[str] = Field(description=\"List of positive aspects\")\n",
    "    cons: List[str] = Field(description=\"List of negative aspects\")\n",
    "    recommended: bool = Field(description=\"Whether you'd recommend it\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=MovieReview)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Write a review for the movie '{movie}'.\\n{format_instructions}\",\n",
    "    input_variables=[\"movie\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "instructions = prompt.format(movie=\"The Matrix but everyone is a rubber duck\")\n",
    "print(\"What the LLM received\", instructions)\n",
    "response = llm.invoke(instructions)\n",
    "parsed_review = parser.parse(response.content)\n",
    "\n",
    "print(\"Structured review:\")\n",
    "print(f\"Title: {parsed_review.title}\")\n",
    "print(f\"Rating: {parsed_review.rating}/10\")\n",
    "print(f\"Pros: {parsed_review.pros}\")\n",
    "print(f\"Cons: {parsed_review.cons}\")\n",
    "print(f\"Recommended: {parsed_review.recommended}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Bizarre News Summarizer\n",
    "\n",
    "Create a system that loads weird Wikipedia pages and generates structured summaries with conspiracy-theory-style interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# TODO: Define your conspiracy theory summary structure\n",
    "class ConspiracySummary(BaseModel):\n",
    "    # YOUR CODE HERE: Add fields for:\n",
    "    # - title: str\n",
    "    # - real_summary: str (actual factual summary)\n",
    "    # - conspiracy_theory: str (humorous conspiracy interpretation)\n",
    "    # - evidence_points: List[str] (\"evidence\" for the conspiracy)\n",
    "    # - danger_level: int (1-10 scale)\n",
    "    pass\n",
    "\n",
    "# TODO: Create parser and prompt template\n",
    "\n",
    "def analyze_weird_topic(wikipedia_url):\n",
    "    \"\"\"Load a Wikipedia page and generate a conspiracy analysis\"\"\"\n",
    "    # YOUR CODE HERE:\n",
    "    # 1. Load the webpage\n",
    "    # 2. Use your structured parser to analyze it\n",
    "    # 3. Return the parsed result\n",
    "    pass\n",
    "\n",
    "# Test with these weird Wikipedia topics:\n",
    "weird_topics = [\n",
    "    \"https://en.wikipedia.org/wiki/Cargo_cult\",\n",
    "    \"https://en.wikipedia.org/wiki/Kentucky_meat_shower\",\n",
    "    \"https://en.wikipedia.org/wiki/Dancing_plague_of_1518\"\n",
    "]\n",
    "\n",
    "# analyze_weird_topic(weird_topics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Rubber duck debugging\n",
      "Real Summary: Rubber duck debugging is a software engineering debugging method in which a programmer explains their code, step by step, to a rubber duck to reveal mistakes and misunderstandings. The technique externalizes the programmer's thought process, helping identify logical errors that might be hidden during silent review.\n",
      "Conspiracy: A silly, fictional conspiracy claims that rubber ducks are secretly coordinating debugging across the software industry. According to the theory, a hidden guild of 'Quack Debuggers' uses squeaks encoded in test outputs and desk duck positions to guide code toward more robust designs, with the Pragmatic Programmer anecdote acting as recruitment lore for this covert order. Every time a coder explains code to a duck, tiny algorithmic nudges are allegedly seeded into the project, while ducks communicate via a secret bathtime dashboard only accessible to those who own a rubber duck.\n",
      "Evidence: ['Ducks on developer desks are routinely oriented to face the monitor when big bugs are being hunted, as if awaiting orders.', 'Internal memos and blog links about the duck anecdote appear with unusual frequency, as if citing a sanctioned ritual rather than a quirky story.', 'CI logs occasionally show squeak-like patterns during failures, which conspiracists claim are secret duck signals.', \"Special edition ducks with insignia and tiny capes are marketed as 'agent ducks' in some office supply catalogs, fueling the myth.\", \"Teams report productivity bumps on days when a new duck joins the desk, hinting at a systemic 'duck deployment' program.\"]\n",
      "Danger Level: 4/10\n"
     ]
    }
   ],
   "source": [
    "class ConspiracySummary(BaseModel):\n",
    "    title: str = Field(description=\"Topic title\")\n",
    "    real_summary: str = Field(description=\"Factual 2-sentence summary\")\n",
    "    conspiracy_theory: str = Field(description=\"Humorous conspiracy interpretation\")\n",
    "    evidence_points: List[str] = Field(description=\"List of 'evidence' supporting the conspiracy\")\n",
    "    danger_level: int = Field(description=\"Danger level from 1-10\")\n",
    "\n",
    "conspiracy_parser = PydanticOutputParser(pydantic_object=ConspiracySummary)\n",
    "\n",
    "conspiracy_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Analyze this content and create both a factual summary and a humorous conspiracy theory interpretation:\n",
    "    \n",
    "    {content}\n",
    "    \n",
    "    {format_instructions}\n",
    "    \n",
    "    Make the conspiracy theory silly but creative. Include fake \"evidence\" points.\n",
    "    \"\"\",\n",
    "    input_variables=[\"content\"],\n",
    "    partial_variables={\"format_instructions\": conspiracy_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "def analyze_weird_topic(wikipedia_url):\n",
    "    loader = WebBaseLoader(wikipedia_url)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # Use first 2000 characters to avoid token limits\n",
    "    content = docs[0].page_content[:2000]\n",
    "    \n",
    "    response = llm.invoke(conspiracy_prompt.format(content=content))\n",
    "    return conspiracy_parser.parse(response.content)\n",
    "\n",
    "# Test it\n",
    "result = analyze_weird_topic(\"https://en.wikipedia.org/wiki/Rubber_duck_debugging\")\n",
    "print(f\"Title: {result.title}\")\n",
    "print(f\"Real Summary: {result.real_summary}\")\n",
    "print(f\"Conspiracy: {result.conspiracy_theory}\")\n",
    "print(f\"Evidence: {result.evidence_points}\")\n",
    "print(f\"Danger Level: {result.danger_level}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Data Loaders**: Get content from web pages, YouTube, RSS feeds, and more.\n",
    "\n",
    "**Output Parsers**: Transform unstructured AI responses into structured data you can actually use in applications.\n",
    "\n",
    "**Key Patterns**:\n",
    "- Use Pydantic models to define your desired output structure\n",
    "- Include format instructions in your prompts\n",
    "- Handle loading errors gracefully\n",
    "- Truncate content to avoid token limits\n",
    "\n",
    "Next up: Chat models, agents, and vectorstores!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
