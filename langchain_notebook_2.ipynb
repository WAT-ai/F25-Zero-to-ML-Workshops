{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Workshop 2: Data Loaders and Output Parsers\n",
    "\n",
    "In this notebook, we'll explore:\n",
    "1. Loading data from interesting sources (YouTube, news, web pages)\n",
    "2. Using output parsers to get structured responses\n",
    "3. Building practical applications that combine both\n",
    "\n",
    "## Setup: Loading Our API Keys Securely üîê\n",
    "\n",
    "First, let's load our environment variables. \n",
    "\n",
    "In real life, **NEVER** put API keys directly in your code! For this Kaggle workshop only, you can set your API key in the `KAGGLE_BACKUP` variable below. Though locally, you should use a `.env` file with the following content: \n",
    "```.env\n",
    "OPENAI_API_KEY=\"your-key\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only if running this on Kaggle\n",
    "KAGGLE_BACKUP = \"sk-...\"  # Replace with your OpenAI key for Kaggle only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import requests\n",
    "    import langchain\n",
    "    import langchain_openai\n",
    "    import langchain_community\n",
    "    from dotenv import load_dotenv\n",
    "    import bs4\n",
    "    import langchain_yt_dlp\n",
    "    import feedparser\n",
    "    import tqdm\n",
    "    from newspaper import Article\n",
    "except ImportError as e:\n",
    "    !pip install requests python-dotenv langchain-openai langchain langchain-community beautifulsoup4 feedparser langchain-yt-dlp newspaper3k listparser lxml-html-clean tqdm\n",
    "    import requests\n",
    "    import langchain\n",
    "    import langchain_openai\n",
    "    import langchain_community\n",
    "    from dotenv import load_dotenv\n",
    "    import bs4\n",
    "    import langchain_yt_dlp\n",
    "    import feedparser\n",
    "    from newspaper import Article\n",
    "    import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load API key with Kaggle backup\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", KAGGLE_BACKUP)\n",
    "if api_key:\n",
    "    print(f\"‚úÖ API key loaded successfully: {api_key[:12]}...\")\n",
    "else:\n",
    "    print(\"‚ùå No API key found. Make sure you have a .env file with OPENAI_API_KEY=\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano\", temperature=0.3, api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Loaders - Getting Data from the Wild\n",
    "\n",
    "Loading data from websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Load a Wikipedia page about something random \n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Special:Random\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Clean up page content to remove extra whitespace\n",
    "for doc in docs:\n",
    "    doc.page_content = \" \".join(doc.page_content.split())\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "print(f\"Content length: {len(docs[0].page_content)} characters\")\n",
    "print(\"\\nStart of article:\")\n",
    "print(docs[0].page_content[500:3000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question about the loaded content\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Use only start of article to keep token limits low. \n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Based on this content, answer in 1-4 sentences: {question}\\n\\nContent: {content}\"\n",
    ")\n",
    "\n",
    "question = \"What is this article about?\"\n",
    "response = llm.invoke(summary_prompt.format(\n",
    "    question=question,\n",
    "    content=docs[0].page_content[500:3000]\n",
    "))\n",
    "\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from Youtube videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YouTube transcript loader (requires youtube-transcript-api)\n",
    "from langchain_yt_dlp.youtube_loader import YoutubeLoaderDL\n",
    "\n",
    "# Load any Youtube video\n",
    "youtube_loader = YoutubeLoaderDL.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",  # Replace with actual video\n",
    "    add_video_info=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    youtube_docs = youtube_loader.load()\n",
    "\n",
    "    for doc in youtube_docs:\n",
    "        doc.metadata['description'] = \" \".join(doc.metadata['description'].split())\n",
    "\n",
    "    print(f\"YouTube description loaded: {len(youtube_docs[0].metadata['description'])} characters\")\n",
    "    print(\"\\nFirst 300 characters:\")\n",
    "    print(youtube_docs[0].metadata['description'][:300] + \"...\")\n",
    "    print(youtube_docs[0].metadata)\n",
    "except Exception as e:\n",
    "    print(f\"YouTube loading failed (this is common): {e}\")\n",
    "    print(\"We'll use web content instead for the exercises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question if YouTube loading succeeded\n",
    "if 'youtube_docs' in locals() and youtube_docs:\n",
    "    question = \"Summarize the main topic of this video in one sentence.\"\n",
    "    response = llm.invoke(summary_prompt.format(\n",
    "        question=question,\n",
    "        content=youtube_docs[0].metadata['description']\n",
    "    ))\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {response.content}\")\n",
    "else:\n",
    "    print(\"Skipping YouTube Q&A since loading failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSS/News loader\n",
    "from langchain_community.document_loaders import RSSFeedLoader\n",
    "\n",
    "# Load recent news\n",
    "rss_loader = RSSFeedLoader(\n",
    "    urls=[\"https://feeds.bbci.co.uk/news/business/rss.xml?edition=int\"],\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    news_docs = rss_loader.load()\n",
    "    for doc in news_docs:\n",
    "        doc.page_content = \" \".join(doc.page_content.split())\n",
    "\n",
    "    print(f\"Loaded {len(news_docs)} news articles\")\n",
    "    print(\"\\nFirst article title:\", news_docs[0].metadata.get('title', 'No title'))\n",
    "    print(\"Summary:\", news_docs[0].page_content[:200] + \"...\")\n",
    "except Exception as e:\n",
    "    print(f\"RSS loading failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask about the news articles if loading succeeded\n",
    "if 'news_docs' in locals() and news_docs:\n",
    "    # Combine first 3 article titles and summaries, truncated\n",
    "    news_sample = \"\\n\".join([\n",
    "        f\"{doc.metadata.get('title', 'No title')}: {doc.page_content[:500]}\" \n",
    "        for doc in news_docs[:3]\n",
    "    ])\n",
    "    \n",
    "    question = \"What are the main themes in these articles? Use examples from the articles to justify your response.\"\n",
    "    response = llm.invoke(summary_prompt.format(\n",
    "        question=question,\n",
    "        content=news_sample\n",
    "    ))\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {response.content}\")\n",
    "else:\n",
    "    print(\"Skipping RSS Q&A since loading failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Output Parsers - Getting Structured Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured data with Pydantic\n",
    "class MovieReview(BaseModel):\n",
    "    title: str = Field(description=\"Movie title\")\n",
    "    rating: int = Field(description=\"Rating from 1-10\")\n",
    "    pros: List[str] = Field(description=\"List of positive aspects\")\n",
    "    cons: List[str] = Field(description=\"List of negative aspects\")\n",
    "    recommended: bool = Field(description=\"Whether you'd recommend it\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=MovieReview)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Write a review for the movie '{movie}'.\\n{format_instructions}\",\n",
    "    input_variables=[\"movie\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "response = llm.invoke(prompt.format(movie=\"The Matrix but everyone is a rubber duck\"))\n",
    "parsed_review = parser.parse(response.content)\n",
    "\n",
    "print(\"Structured review:\")\n",
    "print(f\"Title: {parsed_review.title}\")\n",
    "print(f\"Rating: {parsed_review.rating}/10\")\n",
    "print(f\"Pros: {parsed_review.pros}\")\n",
    "print(f\"Cons: {parsed_review.cons}\")\n",
    "print(f\"Recommended: {parsed_review.recommended}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Bizarre News Summarizer\n",
    "\n",
    "Create a system that loads weird Wikipedia pages and generates structured summaries with conspiracy-theory-style interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# TODO: Define your conspiracy theory summary structure\n",
    "class ConspiracySummary(BaseModel):\n",
    "    # YOUR CODE HERE: Add fields for:\n",
    "    # - title: str\n",
    "    # - real_summary: str (actual factual summary)\n",
    "    # - conspiracy_theory: str (humorous conspiracy interpretation)\n",
    "    # - evidence_points: List[str] (\"evidence\" for the conspiracy)\n",
    "    # - danger_level: int (1-10 scale)\n",
    "    pass\n",
    "\n",
    "# TODO: Create parser and prompt template\n",
    "\n",
    "def analyze_weird_topic(wikipedia_url):\n",
    "    \"\"\"Load a Wikipedia page and generate a conspiracy analysis\"\"\"\n",
    "    # YOUR CODE HERE:\n",
    "    # 1. Load the webpage\n",
    "    # 2. Use your structured parser to analyze it\n",
    "    # 3. Return the parsed result\n",
    "    pass\n",
    "\n",
    "# Test with these weird Wikipedia topics:\n",
    "weird_topics = [\n",
    "    \"https://en.wikipedia.org/wiki/Cargo_cult\",\n",
    "    \"https://en.wikipedia.org/wiki/Kentucky_meat_shower\",\n",
    "    \"https://en.wikipedia.org/wiki/Dancing_plague_of_1518\"\n",
    "]\n",
    "\n",
    "# analyze_weird_topic(weird_topics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConspiracySummary(BaseModel):\n",
    "    title: str = Field(description=\"Topic title\")\n",
    "    real_summary: str = Field(description=\"Factual 2-sentence summary\")\n",
    "    conspiracy_theory: str = Field(description=\"Humorous conspiracy interpretation\")\n",
    "    evidence_points: List[str] = Field(description=\"List of 'evidence' supporting the conspiracy\")\n",
    "    danger_level: int = Field(description=\"Danger level from 1-10\")\n",
    "\n",
    "conspiracy_parser = PydanticOutputParser(pydantic_object=ConspiracySummary)\n",
    "\n",
    "conspiracy_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Analyze this content and create both a factual summary and a humorous conspiracy theory interpretation:\n",
    "    \n",
    "    {content}\n",
    "    \n",
    "    {format_instructions}\n",
    "    \n",
    "    Make the conspiracy theory silly but creative. Include fake \"evidence\" points.\n",
    "    \"\"\",\n",
    "    input_variables=[\"content\"],\n",
    "    partial_variables={\"format_instructions\": conspiracy_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "def analyze_weird_topic(wikipedia_url):\n",
    "    loader = WebBaseLoader(wikipedia_url)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # Use first 2000 characters to avoid token limits\n",
    "    content = docs[0].page_content[:2000]\n",
    "    \n",
    "    response = llm.invoke(conspiracy_prompt.format(content=content))\n",
    "    return conspiracy_parser.parse(response.content)\n",
    "\n",
    "# Test it\n",
    "result = analyze_weird_topic(\"https://en.wikipedia.org/wiki/Rubber_duck_debugging\")\n",
    "print(f\"Title: {result.title}\")\n",
    "print(f\"Real Summary: {result.real_summary}\")\n",
    "print(f\"Conspiracy: {result.conspiracy_theory}\")\n",
    "print(f\"Evidence: {result.evidence_points}\")\n",
    "print(f\"Danger Level: {result.danger_level}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Data Loaders**: Get content from web pages, YouTube, RSS feeds, and more.\n",
    "\n",
    "**Output Parsers**: Transform unstructured AI responses into structured data you can actually use in applications.\n",
    "\n",
    "**Key Patterns**:\n",
    "- Use Pydantic models to define your desired output structure\n",
    "- Include format instructions in your prompts\n",
    "- Handle loading errors gracefully\n",
    "- Truncate content to avoid token limits\n",
    "\n",
    "Next up: Chat models, agents, and vectorstores!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
