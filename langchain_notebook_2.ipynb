{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Workshop 2: Data Loaders and Output Parsers\n",
    "\n",
    "In this notebook, we'll explore:\n",
    "1. Loading data from interesting sources (YouTube, news, web pages)\n",
    "2. Using output parsers to get structured responses\n",
    "3. Building practical applications that combine both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Loaders - Getting Data from the Wild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web page loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Load a Wikipedia page about something interesting\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Rubber_duck_debugging\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "print(f\"Content length: {len(docs[0].page_content)} characters\")\n",
    "print(\"\\nFirst 300 characters:\")\n",
    "print(docs[0].page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YouTube transcript loader (requires youtube-transcript-api)\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "# Load a programming tutorial or tech talk\n",
    "youtube_loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",  # Replace with actual tech video\n",
    "    add_video_info=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    youtube_docs = youtube_loader.load()\n",
    "    print(f\"YouTube transcript loaded: {len(youtube_docs[0].page_content)} characters\")\n",
    "    print(youtube_docs[0].metadata)\n",
    "except Exception as e:\n",
    "    print(f\"YouTube loading failed (this is common): {e}\")\n",
    "    print(\"We'll use web content instead for the exercises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSS/News loader\n",
    "from langchain_community.document_loaders import RSSFeedLoader\n",
    "\n",
    "# Load recent tech news\n",
    "rss_loader = RSSFeedLoader(\n",
    "    urls=[\"https://feeds.ycombinator.com/news.rss\"]\n",
    ")\n",
    "\n",
    "try:\n",
    "    news_docs = rss_loader.load()\n",
    "    print(f\"Loaded {len(news_docs)} news articles\")\n",
    "    print(\"\\nFirst article title:\", news_docs[0].metadata.get('title', 'No title'))\n",
    "    print(\"Summary:\", news_docs[0].page_content[:200] + \"...\")\n",
    "except Exception as e:\n",
    "    print(f\"RSS loading failed: {e}\")\n",
    "    print(\"Using fallback content...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Output Parsers - Getting Structured Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser, CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple list parser\n",
    "list_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List 5 ridiculous programming language names that don't exist yet.\\n{format_instructions}\",\n",
    "    input_variables=[],\n",
    "    partial_variables={\"format_instructions\": list_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "response = llm.invoke(prompt.format())\n",
    "parsed_list = list_parser.parse(response.content)\n",
    "\n",
    "print(\"Raw response:\", response.content)\n",
    "print(\"\\nParsed list:\", parsed_list)\n",
    "print(\"Type:\", type(parsed_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured data with Pydantic\n",
    "class MovieReview(BaseModel):\n",
    "    title: str = Field(description=\"Movie title\")\n",
    "    rating: int = Field(description=\"Rating from 1-10\")\n",
    "    pros: List[str] = Field(description=\"List of positive aspects\")\n",
    "    cons: List[str] = Field(description=\"List of negative aspects\")\n",
    "    recommended: bool = Field(description=\"Whether you'd recommend it\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=MovieReview)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Write a review for the movie '{movie}'.\\n{format_instructions}\",\n",
    "    input_variables=[\"movie\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "response = llm.invoke(prompt.format(movie=\"The Matrix but everyone is a rubber duck\"))\n",
    "parsed_review = parser.parse(response.content)\n",
    "\n",
    "print(\"Structured review:\")\n",
    "print(f\"Title: {parsed_review.title}\")\n",
    "print(f\"Rating: {parsed_review.rating}/10\")\n",
    "print(f\"Pros: {parsed_review.pros}\")\n",
    "print(f\"Cons: {parsed_review.cons}\")\n",
    "print(f\"Recommended: {parsed_review.recommended}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Bizarre News Summarizer\n",
    "\n",
    "Create a system that loads weird Wikipedia pages and generates structured summaries with conspiracy-theory-style interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# TODO: Define your conspiracy theory summary structure\n",
    "class ConspiracySummary(BaseModel):\n",
    "    # YOUR CODE HERE: Add fields for:\n",
    "    # - title: str\n",
    "    # - real_summary: str (actual factual summary)\n",
    "    # - conspiracy_theory: str (humorous conspiracy interpretation)\n",
    "    # - evidence_points: List[str] (\"evidence\" for the conspiracy)\n",
    "    # - danger_level: int (1-10 scale)\n",
    "    pass\n",
    "\n",
    "# TODO: Create parser and prompt template\n",
    "\n",
    "def analyze_weird_topic(wikipedia_url):\n",
    "    \"\"\"Load a Wikipedia page and generate a conspiracy analysis\"\"\"\n",
    "    # YOUR CODE HERE:\n",
    "    # 1. Load the webpage\n",
    "    # 2. Use your structured parser to analyze it\n",
    "    # 3. Return the parsed result\n",
    "    pass\n",
    "\n",
    "# Test with these weird Wikipedia topics:\n",
    "weird_topics = [\n",
    "    \"https://en.wikipedia.org/wiki/Cargo_cult\",\n",
    "    \"https://en.wikipedia.org/wiki/Kentucky_meat_shower\",\n",
    "    \"https://en.wikipedia.org/wiki/Dancing_plague_of_1518\"\n",
    "]\n",
    "\n",
    "# analyze_weird_topic(weird_topics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConspiracySummary(BaseModel):\n",
    "    title: str = Field(description=\"Topic title\")\n",
    "    real_summary: str = Field(description=\"Factual 2-sentence summary\")\n",
    "    conspiracy_theory: str = Field(description=\"Humorous conspiracy interpretation\")\n",
    "    evidence_points: List[str] = Field(description=\"List of 'evidence' supporting the conspiracy\")\n",
    "    danger_level: int = Field(description=\"Danger level from 1-10\")\n",
    "\n",
    "conspiracy_parser = PydanticOutputParser(pydantic_object=ConspiracySummary)\n",
    "\n",
    "conspiracy_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Analyze this content and create both a factual summary and a humorous conspiracy theory interpretation:\n",
    "    \n",
    "    {content}\n",
    "    \n",
    "    {format_instructions}\n",
    "    \n",
    "    Make the conspiracy theory silly but creative. Include fake \"evidence\" points.\n",
    "    \"\"\",\n",
    "    input_variables=[\"content\"],\n",
    "    partial_variables={\"format_instructions\": conspiracy_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "def analyze_weird_topic(wikipedia_url):\n",
    "    loader = WebBaseLoader(wikipedia_url)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # Use first 2000 characters to avoid token limits\n",
    "    content = docs[0].page_content[:2000]\n",
    "    \n",
    "    response = llm.invoke(conspiracy_prompt.format(content=content))\n",
    "    return conspiracy_parser.parse(response.content)\n",
    "\n",
    "# Test it\n",
    "result = analyze_weird_topic(\"https://en.wikipedia.org/wiki/Rubber_duck_debugging\")\n",
    "print(f\"Title: {result.title}\")\n",
    "print(f\"Real Summary: {result.real_summary}\")\n",
    "print(f\"Conspiracy: {result.conspiracy_theory}\")\n",
    "print(f\"Evidence: {result.evidence_points}\")\n",
    "print(f\"Danger Level: {result.danger_level}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Code Review Bot\n",
    "\n",
    "Build a system that loads code from GitHub and provides structured reviews with personality ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define code review structure\n",
    "class CodeReview(BaseModel):\n",
    "    # YOUR CODE HERE: Add fields for:\n",
    "    # - overall_rating: int (1-10)\n",
    "    # - readability: int (1-10) \n",
    "    # - bugs_found: List[str]\n",
    "    # - improvements: List[str]\n",
    "    # - programmer_personality: str (guess the programmer's personality)\n",
    "    # - coffee_consumption_estimate: str (how much coffee they drink)\n",
    "    pass\n",
    "\n",
    "# Sample code snippets to analyze (you can use these or find GitHub links)\n",
    "code_samples = {\n",
    "    \"messy_python\": \"\"\"\n",
    "def calc(x,y,op):\n",
    "    if op=='+':\n",
    "        return x+y\n",
    "    elif op=='-':\n",
    "        return x-y\n",
    "    elif op=='*':\n",
    "        return x*y\n",
    "    elif op=='/':\n",
    "        if y!=0:\n",
    "            return x/y\n",
    "        else:\n",
    "            return 'error'\n",
    "    else:\n",
    "        return 'invalid'\n",
    "\"\"\",\n",
    "    \"over_engineered\": \"\"\"\n",
    "class AbstractCalculatorFactoryInterface:\n",
    "    def create_calculator(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class CalculatorImplementation:\n",
    "    def __init__(self, operation_strategy):\n",
    "        self.strategy = operation_strategy\n",
    "    \n",
    "    def execute(self, x, y):\n",
    "        return self.strategy.perform_operation(x, y)\n",
    "\n",
    "class AdditionStrategy:\n",
    "    def perform_operation(self, x, y):\n",
    "        return x + y\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "def review_code(code_snippet, code_name):\n",
    "    \"\"\"Analyze code and return structured review\"\"\"\n",
    "    # YOUR CODE HERE:\n",
    "    # 1. Create parser and prompt\n",
    "    # 2. Analyze the code\n",
    "    # 3. Return structured review with personality insights\n",
    "    pass\n",
    "\n",
    "# Test both code samples\n",
    "# for name, code in code_samples.items():\n",
    "#     print(f\"\\n=== Reviewing {name} ===\")\n",
    "#     review = review_code(code, name)\n",
    "#     print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeReview(BaseModel):\n",
    "    overall_rating: int = Field(description=\"Overall code quality rating 1-10\")\n",
    "    readability: int = Field(description=\"Code readability rating 1-10\")\n",
    "    bugs_found: List[str] = Field(description=\"List of potential bugs or issues\")\n",
    "    improvements: List[str] = Field(description=\"Suggested improvements\")\n",
    "    programmer_personality: str = Field(description=\"Guess about programmer's personality\")\n",
    "    coffee_consumption_estimate: str = Field(description=\"Estimated daily coffee consumption\")\n",
    "\n",
    "code_parser = PydanticOutputParser(pydantic_object=CodeReview)\n",
    "\n",
    "code_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Review this code and provide analysis. Be thorough but humorous with personality assessment:\n",
    "    \n",
    "    Code name: {code_name}\n",
    "    \n",
    "    ```python\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "    {format_instructions}\n",
    "    \n",
    "    Be creative with the personality and coffee estimates based on coding style.\n",
    "    \"\"\",\n",
    "    input_variables=[\"code\", \"code_name\"],\n",
    "    partial_variables={\"format_instructions\": code_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "def review_code(code_snippet, code_name):\n",
    "    response = llm.invoke(code_prompt.format(code=code_snippet, code_name=code_name))\n",
    "    return code_parser.parse(response.content)\n",
    "\n",
    "# Test both samples\n",
    "for name, code in code_samples.items():\n",
    "    print(f\"\\n=== Reviewing {name} ===\")\n",
    "    review = review_code(code, name)\n",
    "    print(f\"Rating: {review.overall_rating}/10\")\n",
    "    print(f\"Readability: {review.readability}/10\")\n",
    "    print(f\"Bugs: {review.bugs_found}\")\n",
    "    print(f\"Improvements: {review.improvements}\")\n",
    "    print(f\"Personality: {review.programmer_personality}\")\n",
    "    print(f\"Coffee: {review.coffee_consumption_estimate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Data Loaders**: Get content from web pages, YouTube, RSS feeds, and more.\n",
    "\n",
    "**Output Parsers**: Transform unstructured AI responses into structured data you can actually use in applications.\n",
    "\n",
    "**Key Patterns**:\n",
    "- Use Pydantic models to define your desired output structure\n",
    "- Include format instructions in your prompts\n",
    "- Handle loading errors gracefully\n",
    "- Truncate content to avoid token limits\n",
    "\n",
    "Next up: Chat models, agents, and vectorstores!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}