{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Workshop 1: From Manual API Calls to LangChain Magic ‚ú®\n",
    "\n",
    "Welcome to your first LangChain adventure! In this notebook, we'll:\n",
    "1. See how tedious it is to make raw API calls to language models\n",
    "2. Discover how LangChain makes everything easier and more elegant\n",
    "3. Learn about prompt templates and LLM chains\n",
    "4. Build something fun along the way!\n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Loading Our API Keys Securely üîê\n",
    "\n",
    "First, let's load our environment variables. \n",
    "\n",
    "In real life, **NEVER** put API keys directly in your code! For this Kaggle workshop only, you can set your API key in the `KAGGLE_BACKUP` variable below. Though locally, you should use a `.env` file with the following content: \n",
    "```.env\n",
    "OPENAI_API_KEY=\"your-key\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only if running this on Kaggle\n",
    "KAGGLE_BACKUP = \"sk-...\"  # Replace with your actual key for Kaggle only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that libraries exist\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "    import langchain\n",
    "    import langchain_openai\n",
    "    from dotenv import load_dotenv\n",
    "except ImportError as e:\n",
    "    !pip install requests python-dotenv langchain-openai langchain\n",
    "    import requests\n",
    "    import langchain\n",
    "    import langchain_openai\n",
    "    from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify our API key is loaded (we'll only show the first few characters for security)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", KAGGLE_BACKUP)\n",
    "if api_key:\n",
    "    print(f\"‚úÖ API key loaded successfully: {api_key[:12]}...\")\n",
    "else:\n",
    "    print(\"‚ùå No API key found. Make sure you have a .env file with OPENAI_API_KEY=\")\n",
    "    print(\"   Example .env file contents:\")\n",
    "    print(\"   OPENAI_API_KEY=sk-your-key-here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Old Way - Manual API Calls üò§\n",
    "\n",
    "Let's see what it takes to manually call the OpenAI API. Spoiler alert: it's not fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def manual_openai_call(prompt, temperature=0.7, max_tokens=150):\n",
    "    \"\"\"\n",
    "    Make a manual API call to OpenAI - the hard way!\n",
    "    Look at all this boilerplate code we have to write...\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up headers\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Create the payload\n",
    "    payload = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Make the API call\n",
    "        response = requests.post(\n",
    "            \"https://api.openai.com/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=payload\n",
    "        )\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the response\n",
    "        result = response.json()\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "    except KeyError as e:\n",
    "        return f\"Error parsing response: {str(e)}\"\n",
    "\n",
    "# Test our manual function\n",
    "prompt = \"Write a haiku about debugging code\"\n",
    "response = manual_openai_call(prompt)\n",
    "print(\"Manual API call result:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üò± Look at all that code!\n",
    "\n",
    "That was a lot of work just to ask a simple question! We had to:\n",
    "- Set up headers manually\n",
    "- Create the correct payload structure\n",
    "- Handle HTTP requests\n",
    "- Parse JSON responses\n",
    "- Handle errors\n",
    "\n",
    "And this is just for one simple call. Imagine if we wanted to:\n",
    "- Chain multiple AI operations together\n",
    "- Use different models\n",
    "- Add memory to conversations\n",
    "- Connect to different data sources\n",
    "\n",
    "The complexity would explode! üí•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The LangChain Way - Simple and Elegant ‚ú®\n",
    "\n",
    "Now let's see how LangChain makes this SO much easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create an LLM instance - that's it!\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=150,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Make the same call - but so much simpler!\n",
    "response = llm.invoke(\"Write a haiku about debugging code\")\n",
    "print(\"LangChain result:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéâ Much Better!\n",
    "\n",
    "Look how much simpler that was! LangChain handled all the:\n",
    "- API authentication\n",
    "- Request formatting\n",
    "- Response parsing\n",
    "- Error handling\n",
    "\n",
    "We can focus on what matters: **building cool AI applications!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Prompt Templates - Making Prompts Reusable üìù\n",
    "\n",
    "What if we want to ask similar questions with different inputs? Like different user queries?That's where prompt templates shine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a template for generating creative writing\n",
    "creative_template = PromptTemplate(\n",
    "    input_variables=[\"genre\", \"character\", \"setting\"],\n",
    "    template=\"\"\"\n",
    "    Write a short {genre} story (2-3 sentences) featuring:\n",
    "    - Character: {character}\n",
    "    - Setting: {setting}\n",
    "    \n",
    "    Make it creative and engaging!\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# TODO: Fill in the blanks to generate multiple stories\n",
    "scenarios = [\n",
    "    {\"genre\": \"sci-fi\", \"character\": \"a robot barista\", \"setting\": \"Mars colony\"},\n",
    "    {\"genre\": \"____\", \"character\": \"____\", \"setting\": \"____\"},\n",
    "    {\"genre\": \"____\", \"character\": \"____\", \"setting\": \"____\"}\n",
    "]\n",
    "\n",
    "print(\"üé≠ Generated Stories:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, scenario in enumerate(scenarios, 1):\n",
    "    # Format the prompt with our variables\n",
    "    formatted_prompt = creative_template.format(**scenario)\n",
    "    \n",
    "    # Get the AI's response\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    print(f\"Story #{i} ({scenario['genre'].title()}):\")\n",
    "    print(response.content)\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Why Prompt Templates Rock:\n",
    "\n",
    "1. **Reusability**: Write once, use many times with different inputs\n",
    "2. **Consistency**: Same format every time\n",
    "3. **Maintainability**: Easy to update prompts in one place\n",
    "4. **Flexibility**: Mix and match variables easily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: LLM Chains - Connecting Operations Together üîó\n",
    "\n",
    "Now for the real magic: chaining operations together! Let's build a \"Story Critic\" that first generates a story, then critiques it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Template for generating stories\n",
    "story_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Write a very short story (3-4 sentences) about {topic}. Make it interesting!\"\n",
    ")\n",
    "\n",
    "# Template for critiquing stories\n",
    "critic_template = PromptTemplate(\n",
    "    input_variables=[\"story\"],\n",
    "    template=\"\"\"\n",
    "    As a literary critic, provide a brief review of this story:\n",
    "    \n",
    "    \"{story}\"\n",
    "    \n",
    "    Give it a rating out of 5 stars and explain why. Be constructive but honest!\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create chains\n",
    "story_chain = LLMChain(llm=llm, prompt=story_template)\n",
    "critic_chain = LLMChain(llm=llm, prompt=critic_template)\n",
    "\n",
    "def story_and_critique(topic):\n",
    "    \"\"\"Generate a story and then critique it - a two-step AI process!\"\"\"\n",
    "    \n",
    "    print(f\"üìö Generating story about: {topic}\")\n",
    "    story = story_chain.run(topic=topic)\n",
    "    print(\"\\nGenerated Story:\")\n",
    "    print(story)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"üé≠ Now getting a critique...\")\n",
    "    critique = critic_chain.run(story=story)\n",
    "    print(\"\\nCritique:\")\n",
    "    print(critique)\n",
    "    \n",
    "    return story, critique\n",
    "\n",
    "# Try our story critic!\n",
    "story_and_critique(\"a programmer who discovers their code has become sentient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 1: Build Your Own Chain\n",
    "\n",
    "Your turn! Create a \"Product Idea Generator and Validator\" that:\n",
    "1. Takes a market/industry as input\n",
    "2. Generates a creative product idea for that market\n",
    "3. Then analyzes the pros and cons of that product idea\n",
    "\n",
    "Fill in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your templates and chains here!\n",
    "\n",
    "# Template for generating product ideas\n",
    "product_idea_template = PromptTemplate(\n",
    "    input_variables=[\"market\"],\n",
    "    template=\"\"\"\n",
    "    # YOUR CODE HERE: Create a template that generates creative product ideas\n",
    "    # for a given market/industry. Bonus points for funny ideas\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Template for analyzing product ideas\n",
    "analysis_template = PromptTemplate(\n",
    "    input_variables=[\"product_idea\"],\n",
    "    template=\"\"\"\n",
    "    # YOUR CODE HERE: Create a template that analyzes a product idea\n",
    "    # Include pros, cons, and feasibility assessment\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# TODO: Create your chains and test function\n",
    "\n",
    "def generate_and_analyze_product(market):\n",
    "    \"\"\"Generate a product idea and then analyze it\"\"\"\n",
    "    # YOUR CODE HERE: Implement the two-step process\n",
    "    pass\n",
    "\n",
    "# Test your function!\n",
    "# generate_and_analyze_product(\"pet care\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Solution (Don't peek until you try!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1\n",
    "\n",
    "product_idea_template = PromptTemplate(\n",
    "    input_variables=[\"market\"],\n",
    "    template=\"\"\"\n",
    "    Generate a creative and innovative product idea for the {market} market.\n",
    "    Think outside the box! The product should solve a real problem but in a fun or unexpected way.\n",
    "    Describe the product in 2-3 sentences.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "analysis_template = PromptTemplate(\n",
    "    input_variables=[\"product_idea\"],\n",
    "    template=\"\"\"\n",
    "    Analyze this product idea as a business consultant:\n",
    "    \n",
    "    \"{product_idea}\"\n",
    "    \n",
    "    Provide:\n",
    "    1. Top 3 pros/advantages\n",
    "    2. Top 3 cons/challenges  \n",
    "    3. Feasibility score (1-10) with brief explanation\n",
    "    4. One recommendation for improvement\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "product_chain = LLMChain(llm=llm, prompt=product_idea_template)\n",
    "analysis_chain = LLMChain(llm=llm, prompt=analysis_template)\n",
    "\n",
    "def generate_and_analyze_product(market):\n",
    "    print(f\"üí° Generating product idea for: {market}\")\n",
    "    product_idea = product_chain.run(market=market)\n",
    "    print(\"\\nProduct Idea:\")\n",
    "    print(product_idea)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üìä Analyzing the idea...\")\n",
    "    analysis = analysis_chain.run(product_idea=product_idea)\n",
    "    print(\"\\nAnalysis:\")\n",
    "    print(analysis)\n",
    "    \n",
    "    return product_idea, analysis\n",
    "\n",
    "# Test the solution\n",
    "generate_and_analyze_product(\"YOUR PRODUCT IDEA HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 2: The Emoji Translator\n",
    "\n",
    "Let's build something fun! Create an \"Emoji Translator\" that:\n",
    "1. Takes normal text as input\n",
    "2. Converts it to an emoji-heavy version\n",
    "3. Then translates it back to normal English\n",
    "4. Compares how close the round-trip translation is to the original\n",
    "\n",
    "This will be a 3-step chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build your 3-step Emoji Translator!\n",
    "\n",
    "# Step 1: Convert text to emojis\n",
    "to_emoji_template = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"\"\"\n",
    "    # YOUR CODE HERE: Create a template that converts normal text to emoji-heavy text\n",
    "    # Be creative with emoji usage!\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Step 2: Convert emojis back to text\n",
    "from_emoji_template = PromptTemplate(\n",
    "    input_variables=[\"emoji_text\"],\n",
    "    template=\"\"\"\n",
    "    # YOUR CODE HERE: Create a template that interprets emoji text back to normal English\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# TODO: Create your chains and main function\n",
    "\n",
    "def emoji_round_trip(text):\n",
    "    \"\"\"Take text on a round trip through emoji land!\"\"\"\n",
    "    # Step 1: Convert text to emojis\n",
    "    \n",
    "    # Step 2: Convert emojis back to text\n",
    "        \n",
    "    # Step 3: Print the results\n",
    "\n",
    "# Test cases\n",
    "test_sentences = [\n",
    "    \"I love programming and drinking coffee while debugging code\",\n",
    "    \"The weather is sunny and I want to go to the beach\",\n",
    "    \"My cat is sleeping on my laptop keyboard again\"\n",
    "]\n",
    "\n",
    "# Uncomment to test:\n",
    "# for sentence in test_sentences:\n",
    "#     emoji_round_trip(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Solution (Try it yourself first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 2\n",
    "\n",
    "to_emoji_template = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"\"\"\n",
    "    Convert this text to a version that uses lots of emojis while keeping the core meaning:\n",
    "    \n",
    "    \"{text}\"\n",
    "    \n",
    "    Use emojis creatively to represent words, concepts, and emotions. Have fun with it!\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "from_emoji_template = PromptTemplate(\n",
    "    input_variables=[\"emoji_text\"],\n",
    "    template=\"\"\"\n",
    "    Interpret this emoji-heavy text back into normal English:\n",
    "    \n",
    "    \"{emoji_text}\"\n",
    "    \n",
    "    Try to understand what the emojis represent and write it as a clear English sentence.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create chains\n",
    "to_emoji_chain = LLMChain(llm=llm, prompt=to_emoji_template)\n",
    "from_emoji_chain = LLMChain(llm=llm, prompt=from_emoji_template)\n",
    "\n",
    "def emoji_round_trip(text):\n",
    "    print(f\"üîÑ Starting with: {text}\")\n",
    "    \n",
    "    # Step 1: Convert to emoji\n",
    "    print(\"\\nüìù Step 1: Converting to emojis...\")\n",
    "    emoji_version = to_emoji_chain.run(text=text)\n",
    "    print(f\"Emoji version: {emoji_version}\")\n",
    "    \n",
    "    # Step 2: Convert back to text\n",
    "    print(\"\\nüîÑ Step 2: Converting back to text...\")\n",
    "    back_to_text = from_emoji_chain.run(emoji_text=emoji_version)\n",
    "    print(f\"Back to text: {back_to_text}\")\n",
    "    \n",
    "    return emoji_version, back_to_text\n",
    "\n",
    "# Test it out!\n",
    "test_sentences = [\n",
    "    \"I love programming and drinking coffee while debugging code\",\n",
    "    \"The weather is sunny and I want to go to the beach\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    emoji_round_trip(sentence)\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ What We've Learned\n",
    "\n",
    "Congratulations! You've just learned the foundations of LangChain:\n",
    "\n",
    "### ‚úÖ Key Concepts Covered:\n",
    "1. **Manual API Calls vs LangChain**: Saw how much boilerplate code LangChain eliminates\n",
    "2. **LLM Wrappers**: Simple, consistent interface to different language models\n",
    "3. **Prompt Templates**: Reusable, parameterized prompts for consistency\n",
    "4. **LLM Chains**: Connecting multiple AI operations in sequence\n",
    "\n",
    "### üöÄ What's Next:\n",
    "- **Notebook 2**: Data loaders from cool sources (YouTube, news, etc.) and output parsers\n",
    "- **Notebook 3**: Advanced features like chat models, agents, and vectorstores\n",
    "- **Exercise 4**: Building a full Streamlit chatbot!\n",
    "\n",
    "### üí° Pro Tips:\n",
    "- Always use environment variables for API keys\n",
    "- Start simple with templates, then build complexity\n",
    "- Chain operations can be as creative as you want\n",
    "- Test each step of your chain individually for debugging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
